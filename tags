!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_PROGRAM_AUTHOR	Darren Hiebert	/dhiebert@users.sourceforge.net/
!_TAG_PROGRAM_NAME	Exuberant Ctags	//
!_TAG_PROGRAM_URL	http://ctags.sourceforge.net	/official site/
!_TAG_PROGRAM_VERSION	5.8	//
ABC	src/policy_functions.py	/^from abc import ABC, abstractmethod$/;"	i
ABC	src/returns.py	/^from abc import ABC, abstractmethod$/;"	i
ALEInterface	src/learner.py	/^from ale_py import ALEInterface, SDL_SUPPORT$/;"	i
AbstractPolicyApproximator	src/policy_functions.py	/^class AbstractPolicyApproximator(ABC):$/;"	c
AbstractReturns	src/learner.py	/^from returns import AbstractReturns, MonteCarloReturns, MonteCarloBaselineReturns$/;"	i
AbstractReturns	src/returns.py	/^class AbstractReturns(ABC):$/;"	c
Adam	src/policy_functions.py	/^from torch.optim import Adam$/;"	i
Any	src/learner.py	/^from typing import List, Tuple, Callable, Any$/;"	i
CNN	src/cnn.py	/^class CNN(nn.Module):$/;"	c
CNN	src/policy_functions.py	/^from cnn import CNN$/;"	i
CNN	src/returns.py	/^from cnn import CNN$/;"	i
Callable	src/learner.py	/^from typing import List, Tuple, Callable, Any$/;"	i
F	src/policy_functions.py	/^import torch.nn.functional as F$/;"	i
LinearDiscretePolicyApproximator	src/learner.py	/^from policy_functions import NNDiscretePolicyApproximator, LinearDiscretePolicyApproximator$/;"	i
LinearDiscretePolicyApproximator	src/policy_functions.py	/^class LinearDiscretePolicyApproximator(AbstractPolicyApproximator):$/;"	c
List	src/learner.py	/^from typing import List, Tuple, Callable, Any$/;"	i
MonteCarloBaselineReturns	src/learner.py	/^from returns import AbstractReturns, MonteCarloReturns, MonteCarloBaselineReturns$/;"	i
MonteCarloBaselineReturns	src/returns.py	/^class MonteCarloBaselineReturns(MonteCarloReturns):$/;"	c
MonteCarloReturns	src/learner.py	/^from returns import AbstractReturns, MonteCarloReturns, MonteCarloBaselineReturns$/;"	i
MonteCarloReturns	src/returns.py	/^class MonteCarloReturns(AbstractReturns):$/;"	c
NNDiscretePolicyApproximator	src/learner.py	/^from policy_functions import NNDiscretePolicyApproximator, LinearDiscretePolicyApproximator$/;"	i
NNDiscretePolicyApproximator	src/policy_functions.py	/^class NNDiscretePolicyApproximator(AbstractPolicyApproximator, nn.Module):$/;"	c
NNGaussianPolicyApproximator	src/policy_functions.py	/^class NNGaussianPolicyApproximator(AbstractPolicyApproximator, nn.Module):$/;"	c
OrderedDict	src/learner.py	/^from collections import OrderedDict$/;"	i
OrderedDict	src/returns.py	/^from collections import OrderedDict$/;"	i
PolicyGradientAgent	src/learner.py	/^class PolicyGradientAgent():$/;"	c
QValueNetworkReturns	src/returns.py	/^class QValueNetworkReturns(ValueNetworkReturns, nn.Module):$/;"	c
REINFORCEAgent	src/learner.py	/^class REINFORCEAgent(PolicyGradientAgent):$/;"	c
ReplayBuffer	src/learner.py	/^from utils import ReplayBuffer$/;"	i
ReplayBuffer	src/utils.py	/^class ReplayBuffer:$/;"	c
SDL_SUPPORT	src/learner.py	/^from ale_py import ALEInterface, SDL_SUPPORT$/;"	i
The Policy Gradient	report/main.tex	/^\\section{The Policy Gradient}$/;"	s
Tuple	src/learner.py	/^from typing import List, Tuple, Callable, Any$/;"	i
Tuple	src/policy_functions.py	/^from typing import Tuple$/;"	i
Tuple	src/utils.py	/^from typing import Tuple$/;"	i
ValueNetworkReturns	src/returns.py	/^class ValueNetworkReturns(AbstractReturns, torch.nn.Module):$/;"	c
VanillaPolicyGradientAgent	src/learner.py	/^class VanillaPolicyGradientAgent(PolicyGradientAgent):$/;"	c
__call__	src/policy_functions.py	/^    def __call__(self, state: np.ndarray) -> Tuple[int, np.ndarray]:$/;"	m	class:LinearDiscretePolicyApproximator	file:
__call__	src/policy_functions.py	/^    def __call__(self, state: np.ndarray) -> Tuple[int, torch.tensor]:$/;"	m	class:NNDiscretePolicyApproximator	file:
__call__	src/policy_functions.py	/^    def __call__(self, state: np.ndarray) -> Tuple[int, torch.tensor]:$/;"	m	class:NNGaussianPolicyApproximator	file:
__call__	src/policy_functions.py	/^    def __call__(self, state:np.ndarray) -> np.ndarray:$/;"	m	class:AbstractPolicyApproximator	file:
__call__	src/returns.py	/^    def __call__(self, **kwargs):$/;"	m	class:AbstractReturns	file:
__call__	src/returns.py	/^    def __call__(self, state):$/;"	m	class:MonteCarloReturns	file:
__call__	src/returns.py	/^    def __call__(self, state):$/;"	m	class:ValueNetworkReturns	file:
__call__	src/returns.py	/^    def __call__(self, state, action):$/;"	m	class:QValueNetworkReturns	file:
__init__	src/cnn.py	/^    def __init__(self, image_dim):$/;"	m	class:CNN
__init__	src/learner.py	/^    def __init__(self, $/;"	m	class:REINFORCEAgent
__init__	src/learner.py	/^    def __init__(self, $/;"	m	class:VanillaPolicyGradientAgent
__init__	src/learner.py	/^    def __init__(self,$/;"	m	class:PolicyGradientAgent
__init__	src/policy_functions.py	/^    def __init__(self, state_dim, action_space, alpha) -> None:$/;"	m	class:AbstractPolicyApproximator
__init__	src/policy_functions.py	/^    def __init__(self, state_dim, action_space, alpha) -> None:$/;"	m	class:LinearDiscretePolicyApproximator
__init__	src/policy_functions.py	/^    def __init__(self,$/;"	m	class:NNDiscretePolicyApproximator
__init__	src/policy_functions.py	/^    def __init__(self,$/;"	m	class:NNGaussianPolicyApproximator
__init__	src/returns.py	/^    def __init__(self, **kwargs) -> None:$/;"	m	class:AbstractReturns
__init__	src/returns.py	/^    def __init__(self, gamma) -> None:$/;"	m	class:MonteCarloReturns
__init__	src/returns.py	/^    def __init__(self, gamma, baseline) -> None:$/;"	m	class:MonteCarloBaselineReturns
__init__	src/returns.py	/^    def __init__(self, input_shape, alpha=1e-3, from_image=False) -> None:$/;"	m	class:ValueNetworkReturns
__init__	src/utils.py	/^    def __init__(self,$/;"	m	class:ReplayBuffer
__len__	src/utils.py	/^    def __len__(self):$/;"	m	class:ReplayBuffer	file:
abstractmethod	src/policy_functions.py	/^from abc import ABC, abstractmethod$/;"	i
abstractmethod	src/returns.py	/^from abc import ABC, abstractmethod$/;"	i
add_transition	src/utils.py	/^    def add_transition(self, s: np.ndarray, a: np.ndarray, r: float,$/;"	m	class:ReplayBuffer
agent	src/learner.py	/^    agent = REINFORCEAgent($/;"	v
ale	src/learner.py	/^ale = ALEInterface()$/;"	v
ale_py	src/learner.py	/^import ale_py$/;"	i
author	setup.py	/^    author='Tom',$/;"	v
author_email	setup.py	/^    author_email='wilson.th@northeastern.edu',$/;"	v
cnn	src/cnn.py	/^    cnn = CNN((180, 210, 3))$/;"	v	class:CNN
compute_output_size	src/cnn.py	/^    def compute_output_size(self, img_shape) :$/;"	m	class:CNN
compute_score	src/policy_functions.py	/^    def compute_score(self, s_t, a_t, log_prob, advantage):$/;"	m	class:NNGaussianPolicyApproximator
compute_score	src/policy_functions.py	/^    def compute_score(self, tau):$/;"	m	class:AbstractPolicyApproximator
compute_score	src/policy_functions.py	/^    def compute_score(self, tau, G_tau):$/;"	m	class:LinearDiscretePolicyApproximator
compute_score	src/policy_functions.py	/^    def compute_score(self, tau, G_tau):$/;"	m	class:NNDiscretePolicyApproximator
compute_step_score	src/policy_functions.py	/^    def compute_step_score(self, s_t, a_t, r, log_prob, G):$/;"	m	class:AbstractPolicyApproximator
compute_step_score	src/policy_functions.py	/^    def compute_step_score(self, s_t, a_t, r, log_prob, G):$/;"	m	class:LinearDiscretePolicyApproximator
compute_step_score	src/policy_functions.py	/^    def compute_step_score(self, s_t, a_t, r, log_prob, G):$/;"	m	class:NNDiscretePolicyApproximator
description	setup.py	/^    description='A simple policy gradients library with support for pytorch and numpy backprop'$/;"	v
env	src/learner.py	/^                env=env)$/;"	v
env	src/learner.py	/^    env = gym.make("LunarLander-v2")$/;"	v
forward	src/cnn.py	/^    def forward(self, x):$/;"	m	class:CNN
forward	src/policy_functions.py	/^    def forward(self, state):$/;"	m	class:LinearDiscretePolicyApproximator
forward	src/policy_functions.py	/^    def forward(self, state):$/;"	m	class:NNDiscretePolicyApproximator
forward	src/policy_functions.py	/^    def forward(self, state):$/;"	m	class:NNGaussianPolicyApproximator
forward	src/returns.py	/^    def forward(self, state, action) -> torch.Tensor:$/;"	m	class:QValueNetworkReturns
forward	src/returns.py	/^    def forward(self, x) -> torch.Tensor:$/;"	m	class:ValueNetworkReturns
gamma	src/learner.py	/^                gamma=0.98,$/;"	v
get_params	src/policy_functions.py	/^    def get_params(self):$/;"	m	class:AbstractPolicyApproximator
get_params	src/policy_functions.py	/^    def get_params(self):$/;"	m	class:LinearDiscretePolicyApproximator
get_params	src/policy_functions.py	/^    def get_params(self):$/;"	m	class:NNDiscretePolicyApproximator
get_params	src/policy_functions.py	/^    def get_params(self):$/;"	m	class:NNGaussianPolicyApproximator
gym	src/learner.py	/^import gym$/;"	i
inp	src/returns.py	/^    inp = torch.randn((1, 64))$/;"	v
license	setup.py	/^    license='',$/;"	v
log_softmax	src/policy_functions.py	/^        def log_softmax(x):$/;"	f	function:LinearDiscretePolicyApproximator.forward
n_epochs	src/learner.py	/^    n_epochs = 2 $/;"	v
name	setup.py	/^    name='sq_anyPG',$/;"	v
net	src/returns.py	/^    net = QValueNetworkReturns(64, from_image=False)$/;"	v
nn	src/cnn.py	/^import torch.nn as nn$/;"	i
nn	src/policy_functions.py	/^import torch.nn as nn$/;"	i
nn	src/policy_functions.py	/^import torch.nn.functional as F$/;"	i
nn	src/returns.py	/^import torch.nn as nn$/;"	i
np	src/cnn.py	/^import numpy as np$/;"	i
np	src/learner.py	/^import numpy as np$/;"	i
np	src/policy_functions.py	/^import numpy as np$/;"	i
np	src/returns.py	/^import numpy as np$/;"	i
np	src/utils.py	/^import numpy as np$/;"	i
obs	src/cnn.py	/^    obs = torch.randn(1, 180, 210, 3)$/;"	v	class:CNN
os	src/learner.py	/^import os$/;"	i
packages	setup.py	/^    packages=[''],$/;"	v
pickle	src/learner.py	/^import pickle$/;"	i
playout	src/learner.py	/^    def playout(self, render:bool=False):$/;"	m	class:PolicyGradientAgent
plot_curves	src/learner.py	/^def plot_curves(rewards, loss):$/;"	f
plt	src/learner.py	/^from matplotlib import pyplot as plt$/;"	i
policy_fn	src/learner.py	/^                policy_fn=policy_fn,$/;"	v
policy_fn	src/learner.py	/^    policy_fn = NNDiscretePolicyApproximator(env.observation_space, env.action_space, alpha=1e-3, from_image=False)$/;"	v
predict	src/returns.py	/^    def predict(self, state: torch.Tensor) -> torch.Tensor:$/;"	m	class:ValueNetworkReturns
preprocess	src/cnn.py	/^    def preprocess(self, obs):$/;"	m	class:CNN
reward_to_go	src/returns.py	/^    def reward_to_go(self, tau):$/;"	m	class:MonteCarloReturns
sample	src/utils.py	/^    def sample(self, batch_size: int) -> Tuple:$/;"	m	class:ReplayBuffer
setup	setup.py	/^from distutils.core import setup$/;"	i
sys	src/learner.py	/^import sys$/;"	i
td_loss	src/returns.py	/^    def td_loss(self, s, a, r, sp, d):$/;"	m	class:ValueNetworkReturns
torch	src/cnn.py	/^import torch$/;"	i
torch	src/cnn.py	/^import torch.nn as nn$/;"	i
torch	src/learner.py	/^import torch$/;"	i
torch	src/policy_functions.py	/^import torch$/;"	i
torch	src/policy_functions.py	/^import torch.nn as nn$/;"	i
torch	src/policy_functions.py	/^import torch.nn.functional as F$/;"	i
torch	src/returns.py	/^import torch$/;"	i
torch	src/returns.py	/^import torch.nn as nn$/;"	i
tqdm	src/learner.py	/^from tqdm import tqdm$/;"	i
train	src/learner.py	/^    def train(self, n_iter):$/;"	m	class:PolicyGradientAgent
train	src/learner.py	/^    def train(self, n_iter):$/;"	m	class:REINFORCEAgent
train	src/learner.py	/^    def train(self, n_iter):$/;"	m	class:VanillaPolicyGradientAgent
update_baseline	src/returns.py	/^    def update_baseline(self, *playouts):$/;"	m	class:MonteCarloBaselineReturns
update_baseline	src/returns.py	/^    def update_baseline(self, *playouts):$/;"	m	class:MonteCarloReturns
update_baseline	src/returns.py	/^    def update_baseline(self, s, a, r, sp, d):$/;"	m	class:ValueNetworkReturns
update_baseline	src/returns.py	/^    def update_baseline(self, tau):$/;"	m	class:AbstractReturns
update_params	src/policy_functions.py	/^    def update_params(self, score):$/;"	m	class:AbstractPolicyApproximator
update_params	src/policy_functions.py	/^    def update_params(self, score):$/;"	m	class:LinearDiscretePolicyApproximator
update_params	src/policy_functions.py	/^    def update_params(self, score):$/;"	m	class:NNDiscretePolicyApproximator
update_params	src/policy_functions.py	/^    def update_params(self, scores):$/;"	m	class:NNGaussianPolicyApproximator
url	setup.py	/^    url='https:\/\/github.com\/tw-ilson\/PolicyGradients',$/;"	v
version	setup.py	/^    version='',$/;"	v
